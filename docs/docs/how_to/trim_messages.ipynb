{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaad9a82-0592-4315-9931-0621054bdd0e",
   "metadata": {},
   "source": [
    "# How to trim messages\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "\n",
    "- [Messages](/docs/concepts/#messages)\n",
    "- [Chat models](/docs/concepts/#chat-models)\n",
    "- [Chaining](/docs/how_to/sequence/)\n",
    "- [Chat history](/docs/concepts/#chat-history)\n",
    "\n",
    "The methods in this guide also require `langchain-core>=0.2.9`.\n",
    "\n",
    ":::\n",
    "\n",
    "All models have finite context windows, meaning there's a limit to how many tokens they can take as input. If you have very long messages or a chain/agent that accumulates a long message is history, you'll need to manage the length of the messages you're passing in to the model.\n",
    "\n",
    "The [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) util provides some basic strategies for trimming a list of messages to a specific size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bffc37-78c0-46c3-ad0c-b44de0ed3e90",
   "metadata": {},
   "source": [
    "## Getting the last `max_tokens` tokens\n",
    "\n",
    "The most common approach to trim the chat history is based on the **token count** across all messages.\n",
    "\n",
    "Below is a configuration of `trim_messages` that:\n",
    "\n",
    "1. Includes recent messages and drops old messages from the chat history. It does so under the constraint that the **token count** across all messages remains below a given threshold.\n",
    "2. Ensures that the new chat history is valid -- most chat models expect that the chat history starts with either (1) a `HumanMessage` or (2) a [SystemMessage](/docs/concepts/#systemmessage) followed by a `HumanMessage`.\n",
    "3. Includes the `SystemMessage` in the new chat history if it was present in the original chat history. The `SystemMessage` is the first message in the history and includes special instructions to the model.\n",
    "\n",
    "To get the last `max_tokens` in the list of Messages we can set `strategy=\"last\"`. Notice that for our `token_counter` we can pass in a function (more on that below) or a language model (since language models have a message token counting method). We'll also set `include_system=True` to include the `SystemMessage`, and `start_on=\"human\"` to make sure the resulting chat history is valid. It makes sense to pass in a model when you're trimming your messages to fit into the context window of that specific model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7053411-e33c-49bc-b5a7-042c11628e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c974633b-3bd0-4844-8a8f-85e3e25f13fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"what do you call a speechless parrot\"),\n",
    "]\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    # Most chat models expect that chat history starts with either: \n",
    "    # (1) a HumanMessage or \n",
    "    # (2) a SystemMessage followed by a HumanMessage\n",
    "    start_on=\"human\",\n",
    "    # Usually, we want to keep the SystemMessage\n",
    "    # if it's present in the original history.\n",
    "    # The SystemMessage has special instructions for the model.\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcfc94-0d4a-415c-9506-8ae7634253a2",
   "metadata": {},
   "source": [
    "## Filtering based on number of messages\n",
    "\n",
    "Another common approach to trim the chat history is based on the **message count**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa47565-0f67-44d7-b6a3-4cbf42f24b00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a8b542c-04d1-4515-8d82-b999ea4fac4f",
   "metadata": {},
   "source": [
    "If we want to allow splitting up the contents of a message we can specify `allow_partial=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c46a209-dddd-4d01-81f6-f6ae55d3225c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=56,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    include_system=True,\n",
    "    allow_partial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bee9b-e515-4e89-8f2a-84bda9a25de8",
   "metadata": {},
   "source": [
    "If you don't want to keep the initial system message, specify `include_system=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94351736-28a1-44a3-aac7-82356c81d171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    include_system=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d391d-235b-4091-b2de-c22866b478f3",
   "metadata": {},
   "source": [
    "## Getting the first `max_tokens` tokens\n",
    "\n",
    "We can perform the flipped operation of getting the *first* `max_tokens` by specifying `strategy=\"first\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f56ae54-1a39-4019-9351-3b494c003d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
       " HumanMessage(content=\"i wonder why it's called langchain\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"first\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70bf70-1e5a-4d51-b9b8-a823bf2cf532",
   "metadata": {},
   "source": [
    "## Writing a custom token counter\n",
    "\n",
    "We can write a custom token counter function that takes in a list of messages and returns an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d930c089-e8e6-4980-9d39-11d41e794772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c1c3b1e-2ece-49e7-a3b6-e69877c1633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import tiktoken\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "\n",
    "def str_token_counter(text: str) -> int:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
    "    \"\"\"Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "\n",
    "    For simplicity only supports str Message.contents.\n",
    "    \"\"\"\n",
    "    num_tokens = 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            role = \"tool\"\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            role = \"system\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n",
    "        num_tokens += (\n",
    "            tokens_per_message\n",
    "            + str_token_counter(role)\n",
    "            + str_token_counter(msg.content)\n",
    "        )\n",
    "        if msg.name:\n",
    "            num_tokens += tokens_per_name + str_token_counter(msg.name)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=tiktoken_counter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a672b-c007-47c5-9105-617944dc0a6a",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "`trim_messages` can be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96aa29b2-01e0-437c-a1ab-02fb0141cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"A *polygon*! Because it's a *parrot-gone*! 😄\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 32, 'total_tokens': 47, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3537616b13', 'finish_reason': 'stop', 'logprobs': None}, id='run-e03ec003-823f-4422-8d46-94283dd5178d-0', usage_metadata={'input_tokens': 32, 'output_tokens': 15, 'total_tokens': 47})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Notice we don't pass in messages. This creates\n",
    "# a RunnableLambda that takes messages as input\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | llm\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91d390-e7f7-467b-ad87-d100411d7a21",
   "metadata": {},
   "source": [
    "Looking at the LangSmith trace we can see that before the messages are passed to the model they are first trimmed: https://smith.langchain.com/public/65af12c4-c24d-4824-90f0-6547566e59bb/r\n",
    "\n",
    "Looking at just the trimmer, we can see that it's a Runnable object that can be invoked like all Runnables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff02d0a-353d-4fac-a77c-7c2c5262abd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4720c8-4062-4ebc-9385-58411202ce6e",
   "metadata": {},
   "source": [
    "## Using with ChatMessageHistory\n",
    "\n",
    "Trimming messages is especially useful when [working with chat histories](/docs/how_to/message_history/), which can get arbitrarily long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9517858-fc2f-4dc3-898d-bf98a0e905a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A \"poly-no-wanna-talk!\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 32, 'total_tokens': 40, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_e375328146', 'finish_reason': 'stop', 'logprobs': None}, id='run-cc8df457-89ea-4dc0-a898-0951a0f5247f-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "chat_history = InMemoryChatMessageHistory(messages=messages[:-1])\n",
    "\n",
    "\n",
    "def dummy_get_session_history(session_id):\n",
    "    if session_id != \"1\":\n",
    "        return InMemoryChatMessageHistory()\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "chain = trimmer | llm\n",
    "chain_with_history = RunnableWithMessageHistory(chain, dummy_get_session_history)\n",
    "chain_with_history.invoke(\n",
    "    [HumanMessage(\"what do you call a speechless parrot\")],\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b7b4c-43cb-41de-94fc-1a41f4ec4d2e",
   "metadata": {},
   "source": [
    "Looking at the LangSmith trace we can see that we retrieve all of our messages but before the messages are passed to the model they are trimmed to be just the system message and last human message: https://smith.langchain.com/public/17dd700b-9994-44ca-930c-116e00997315/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc7b84-b92f-44e7-8beb-ba22398e4efb",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "For a complete description of all arguments head to the API reference: https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
